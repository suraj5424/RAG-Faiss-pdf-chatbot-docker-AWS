# ğŸ¤– RAG Chatbot App
**ğŸ–¥ï¸ Streamlit frontend â€¢ ğŸ Python RAG backend â€¢ ğŸ³ Docker Compose â€¢ ğŸŒ NGINX reverse proxy â€¢ â˜ï¸ AWS EC2 (t3.micro, Free Tier) â€¢ â° cron scheduling**

ğŸ”— **Live App:** [Click here to experience the app](http://13.48.71.0/)

---

## ğŸ¬ Demo Preview ğŸ‘‡

<!-- <img src="https://github.com/suraj5424/RAG-Faiss-pdf-chatbot-docker-AWS/blob/main/demo.gif" alt="Demo of RAG Chatbot Application" style="width:600px; height:auto;" />file -->

### **Reduce the video speed to 0.25Ã—**

https://github.com/user-attachments/assets/cc2f212c-ef5d-4a87-bb59-cf206771e045



This README documents the **entire** RAG Chatbot web application: local development, containerized deployment, NGINX reverse proxy, and a lowâ€‘cost AWS EC2 deployment with scheduled runtime using `cron` to reduce resource usage. Follow this guide to set up, run, deploy, and maintain the app.

---

# ğŸ“Œ 1. Project Title & Description

**Name:** `RAG Chatbot App`

**Short description:**  
A web application that combines retrieval (vector search over ingested documents) with a Large Language Model to produce contextâ€‘aware chat responses (Retrievalâ€‘Augmented Generation â€” RAG). The frontend is a Streamlit chat UI; the backend handles ingestion, embedding generation, vector search, reranking (MMR), and LLM calls. Deployable with Docker Compose behind an NGINX reverse proxy; optimized to run on a Free Tier AWS EC2 `t3.micro` by applying containerâ€‘level resource limits and scheduled runtime.

---

# ğŸ—ï¸ 2. Architecture Overview

## Logical diagram (textual)

```
[User Browser]
      |
   (HTTP 80)
      |
   [NGINX Reverse Proxy] -----> [Streamlit Frontend Container (8501)]
      |
      `----> [Backend Container (8000)] (internal API calls from frontend)
```

* **NGINX** listens on port **80** and forwards traffic to the Streamlit frontend container.
* **Frontend (Streamlit)** provides UI, maintains perâ€‘session state (`st.session_state`), and calls backend endpoints such as `/chat`, `/ingest`, `/summarize`.
* **Backend (FastAPI)** performs document ingestion, text chunking, embedding generation (ONNX MiniLM), stores vectors (FAISS), executes retrieval (including MMR), and routes prompts to an LLM provider (OpenAI/OpenRouter/etc.).
* All services are Dockerized and orchestrated by **Docker Compose**.

## Techâ€‘stack summary

* **Frontend:** Streamlit (Python)
* **Backend:** Python (FastAPI) with LangChainâ€‘style components
* **Embeddings model:** `allâ€‘MiniLMâ€‘L6â€‘v2` (ONNX)
* **Vector store:** FAISS (local)
* **Reverse proxy:** NGINX (Alpine)
* **Container orchestration:** Docker Compose (v3.8)
* **Hosting:** AWS EC2 (Ubuntu 24.04 LTS, `t3.micro`)
* **Scheduling:** `cron` jobs for start/stop to conserve resources
* **Env management:** `.env` file (recommended to keep secrets out of source control)

---

# âœ¨ 3. Features

## Backend

* File & URL ingestion (PDFs, web pages)
* Text chunking + embedding generation (ONNX MiniLM)
* Vector store integration (FAISS local)
* Retriever with MMR reâ€‘ranking and configurable `k`
* LLM integration via provider (OpenAI / OpenRouter / Cerebras)
* API endpoints: `/chat`, `/ingest`, `/summarize`, `/remove_vectors`, `/remove_all_vectors`, `/settings`, `/switch_model`, `/health`

## Frontend

* Streamlit chat UI with `st.chat_input` / `st.chat_message`
* Perâ€‘user session isolation via `st.session_state` and `session_id`
* Upload PDFs or provide URLs to ingest
* Control temperature and embedding model mode
* Display AI responses and retrieved source attributions
* Export chat (JSON / Markdown / PDF)

## Infrastructure

* Docker Compose deployment with containerâ€‘level memory & CPU limits
* NGINX reverse proxy exposes only port **80** (optionally 443)
* Cron jobs to start/stop containers on a schedule to keep costs low

---

# ğŸ§° 4. Technologies Used

* **Languages:** Pythonâ€¯3.10+
* **Frontend:** Streamlit
* **Backend:** FastAPI, LangChainâ€‘style components
* **Embeddings model:** `allâ€‘MiniLMâ€‘L6â€‘v2` (ONNX)
* **Vector store:** FAISS (local)
* **Quantization:** Quantized ONNX model for speed & size reduction
* **LLM providers:** `deepseekâ€‘chatâ€‘v3â€‘0324` (OpenRouter) & Cerebras (`gptâ€‘ossâ€‘120b`)
* **Containers:** Docker, Docker Compose (v3.8)
* **Proxy:** NGINX (Alpine)
* **Hosting:** AWS EC2 (Ubuntuâ€¯24.04â€¯LTS)
* **Scheduling:** `cron`
* **Utilities:** `pythonâ€‘dotenv`, `requests/httpx`, `PyMuPDF/pypdf`, `BeautifulSoup`

---

# ğŸ“‹ 5. System Requirements

**For AWS deployment (recommended minimal config):**

* Instance: `t3.micro` (Free Tier) â€” 2â€¯vCPUs (burstable), 1â€¯GiB RAM
* Storage: EBS gp3/gp2 â€” 30â€¯GB (start)
* OS: Ubuntu Serverâ€¯24.04â€¯LTS (x86_64)
* Docker & Docker Compose installed

**Local development requirements:**

* Pythonâ€¯3.10+
* Streamlit & backend dependencies (`pip install -r requirements.txt`)
* Docker (optional for containerised local testing)

> âš ï¸ Note: The `t3.micro` is memoryâ€‘limited. We use swapâ€¯+â€¯container memory limits; for multiâ€‘user or heavier workloads, upgrade the instance.

---

# ğŸ”§ 6. Installation & Setup

## Clone the repository

```bash
git clone https://github.com/your-repo/rag-chatbot.git
cd rag-chatbot
```

## Project directory layout

```
rag-chatbot/
â”œâ”€ rag_pdfchatbot_backend/   # Python backend (FastAPI)
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ Dockerfile
â”‚  â””â”€ requirements.txt
â”œâ”€ rag_pdfchatbot_frontend/  # Streamlit app
â”‚  â”œâ”€ app.py
â”‚  â”œâ”€ css.py
â”‚  â””â”€ requirements.txt
â”œâ”€ nginx/
â”‚  â”œâ”€ Dockerfile
â”‚  â””â”€ default.conf
â”œâ”€ onnx_model/               # ONNX model files
â”œâ”€ user_data/                # FAISS vector store (created at runtime)
â”œâ”€ docker-compose.yml
â””â”€ .env
```

## Create & populate `.env`

Copy the example and edit with your secrets:

```bash
cp .env.example .env
nano .env
```

### Required variables

```env
OPENROUTER_API_KEY=...
CEREBRAS_API_KEY=...
MODEL_NAME=deepseek-chat-v3-0324   # or any supported model
```

> **Security note:** Do **not** commit `.env` to source control. Use AWS Secrets Manager or Docker secrets for production.

---

# ğŸ³ 7. Docker Compose â€“ `docker-compose.yml`

The current `docker-compose.yml` builds the backend, frontend, and NGINX from the repository:

```yaml
version: '3.8'

services:
  backend:
    build: ./rag_pdfchatbot_backend
    container_name: rag-backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    restart: always
    volumes:
      - ./onnx_model:/app/onnx_model   # model files
      - ./user_data:/app/user_data     # persistent FAISS index
    mem_limit: 600m
    cpus: 0.5

  frontend:
    build: ./rag_pdfchatbot_frontend
    container_name: rag-frontend
    ports:
      - "8501:8501"
    depends_on:
      - backend
    restart: always
    mem_limit: 350m
    cpus: 0.4

  nginx:
    build: ./nginx
    container_name: nginx-reverse-proxy
    ports:
      - "80:80"
    depends_on:
      - frontend
      - backend
    restart: always
    mem_limit: 200m
    cpus: 0.2

networks:
  default:
    driver: bridge
```

**Key points**

* **Build** is used instead of preâ€‘built images â€“ the containers are built from the local `Dockerfile`s.
* **Volumes** mount the ONNX model and a persistent `user_data` directory for the FAISS index.
* **Resource limits** keep the containers within the memory budget of a `t3.micro`.

---

# ğŸŒ 8. NGINX Configuration (`nginx/default.conf`)

```nginx
server {
    listen 80;
    server_name _;  # replace with your.domain.tld if you have one

    # Proxy traffic to the Streamlit frontend
    location / {
        proxy_pass http://frontend:8501;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_cache_bypass $http_upgrade;
    }

    # Health endpoint proxied to the backend
    location /health {
        proxy_pass http://backend:8000/health;
        proxy_set_header Host $host;
    }
}
```

* The configuration forwards all user traffic to the Streamlit container and exposes a `/health` endpoint that forwards to the FastAPI health check.

---

# â° 9. Cron Job Scheduling (Start/Stop)

To minimise AWS costs, the backend runs only during working hours (Monâ€‘Satâ€¯07:00â€‘22:00â€¯Berlin time). The frontend can stay up 24/7 or follow the same schedule.

```cron
# Use Berlin time for schedule
TZ=Europe/Berlin

# Start backend at 07:00 Monâ€‘Sat
0 7 * * 1-6 /usr/bin/docker start rag-backend

# Stop backend at 22:00 Monâ€‘Sat
0 22 * * 1-6 /usr/bin/docker stop rag-backend

# Ensure backend is stopped on Sunday at midnight
0 0 * * 0 /usr/bin/docker stop rag-backend

# Start frontend at 07:00 Monâ€‘Sat (optional â€“ comment out to keep it always running)
0 7 * * 1-6 /usr/bin/docker start rag-frontend
```

* Use full paths (`/usr/bin/docker`) to avoid PATH issues in cron.
* Ensure the `ubuntu` user (or whichever user runs cron) has permission to run Docker commands (add to `docker` group or use `sudo`).

---

# ğŸ” 10. Environment Variables (Detailed)

```env
# LLM provider & keys
MODEL_NAME=deepseek-chat-v3-0324
OPENROUTER_API_KEY=...
CEREBRAS_API_KEY=...

# Host references (service names inside Docker Compose)
BACKEND_HOST=http://backend:8000
FRONTEND_HOST=http://frontend:8501

# App configuration
MAX_UPLOAD_MB=10
RETRIEVER_K=4
RETRIEVER_FETCH_K=10
MMR_LAMBDA=0.7

# Admin / light auth (optional)
ADMIN_API_KEY=changeme
```

**Security note** â€“ keep this file out of version control. In production, store secrets in AWS Secrets Manager or Parameter Store.

---

# ğŸ’¾ 11. Resource Management & Rationale

| Service   | Memory limit | CPU share | Rationale |
|-----------|--------------|----------|-----------|
| backend   | 600â€¯MiB      | 0.5      | Embedding & LLM calls are the heaviest; limit prevents OOM on `t3.micro`. |
| frontend  | 350â€¯MiB      | 0.4      | Streamlit is lightweight but needs headroom for session data. |
| nginx     | 200â€¯MiB      | 0.2      | NGINX is very lightweight; limit is generous. |

With 1â€¯GiB RAM on the instance and a swap file (recommended 1â€¯GiB), the containers stay within the budget while leaving room for OS overhead.

---

# ğŸš€ 12. Usage â€“ Running & Accessing

## Local (nonâ€‘Docker) quick dev

```bash
# Backend
cd rag_pdfchatbot_backend
pip install -r requirements.txt
uvicorn main:app --reload

# Frontend
cd rag_pdfchatbot_frontend
pip install -r requirements.txt
streamlit run app.py
```

Set `BACKEND_HOST` in `.env` to `http://localhost:8000` for local testing.

## Containerised (recommended)

```bash
docker-compose up -d --build
```

### Verify containers

```bash
docker ps
docker logs -f rag-frontend
docker logs -f rag-backend
docker logs -f nginx-reverse-proxy
```

### Access the app

* Public URL: `http://<EC2_PUBLIC_IP>/` (NGINX forwards to Streamlit)
* Health check: `http://<EC2_PUBLIC_IP>/health`

### Restart / stop

```bash
docker-compose restart backend
docker-compose stop backend
docker-compose up -d
```

---

# ğŸ§ª 13. API / Frontend Integration (contract summary)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/chat` | `POST` | Chat with the RAG bot. |
| `/ingest` | `POST` | Upload a PDF or provide a URL to ingest. |
| `/summarize` | `POST` | Generate a summary of a PDF or URL (does **not** store vectors). |
| `/remove_vectors` | `DELETE` | Delete vectors for a specific source. |
| `/remove_all_vectors` | `DELETE` | Delete **all** vectors for the session. |
| `/settings` | `GET/POST` | Get or update session settings (e.g., temperature). |
| `/switch_model` | `POST` | Switch between quantised / regular ONNX model. |
| `/health` | `GET` | Simple health check (used by NGINX). |

All endpoints require a `session_id` query parameter (UUIDâ€‘style) to isolate user data.

---

# ğŸ“œ 14. Contribution, License & Credits

* **License** â€“ MIT (see `LICENSE` file).  
* **Contributions** â€“ Fork the repo, create a feature branch, and open a Pull Request.  
* **Credits** â€“ Built with Streamlit, FastAPI, LangChainâ€‘style components, Docker, and NGINX. Hosted on AWS EC2.

---

# âœ… 15. Quick Deploy Checklist

1. Launch an AWS `t3.micro` instance (Ubuntuâ€¯24.04) and open portsâ€¯22â€¯(SSH) andâ€¯80â€¯(HTTP).  
2. SSH into the instance and install Docker & Docker Compose.  
3. Clone the repository into `/home/ubuntu/rag-chatbot`.  
4. Copy `.env.example` â†’ `.env` and fill in API keys.  
5. (Recommended) Create a 1â€¯GiB swap file to avoid OOM.  
6. Ensure `nginx/default.conf` and `docker-compose.yml` are present.  
7. Run `docker-compose up -d --build`.  
8. Add the cron entries from sectionâ€¯9.  
9. Verify the site at `http://<EC2_PUBLIC_IP>/`.  
10. (Optional) Add TLS via Let's Encrypt and harden security groups.  
11. Monitor logs; adjust `mem_limit`/`cpus` if needed.

---

# ğŸ“š 16. Further Enhancements (next steps)

* **Authentication** â€“ JWT or APIâ€‘key based auth for the backend.  
* **Scalable vector store** â€“ Replace local FAISS with Qdrant or Pinecone for multiâ€‘instance scaling.  
* **Observability** â€“ Add Prometheus + Grafana dashboards; ship logs to CloudWatch.  
* **CI/CD** â€“ GitHub Actions to build Docker images and deploy automatically to EC2.  
* **TLS** â€“ Automate HTTPS with Certbot (Let's Encrypt) behind NGINX.

---

## ğŸ™‹ Author & Contact

**Author:** Surajâ€¯Varma  
**Email:** sv3677781@gmail.com  
**GitHub:** [@suraj5424](https://github.com/suraj5424)  
**LinkedIn:** [Surajâ€¯Varma](https://www.linkedin.com/in/suraj5424/)  
**Portfolio:** [surajâ€‘varma.vercel.app](https://suraj-varma.vercel.app/)

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---

## â­ Acknowledgments

* Streamlit â€“ UI framework  
* FastAPI â€“ backend framework  
* Docker â€“ containerisation  
* NGINX â€“ reverse proxy  
* AWS EC2 â€“ hosting

---

## ğŸ™ Support

If you find this project useful, please give it a â­ on GitHub!
